\documentclass[sigplan]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{graphicx,hyperref}
\usepackage{float}
\usepackage{proof,tikz}
\usepackage{amssymb,amsthm,stmaryrd}

\usetikzlibrary{arrows}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[SBLP2018]{XXII BRAZILIAN SYMPOSIUM ON PROGRAMMING LANGUAGES}{September 17--21, 2018}{S\~ao Carlos}
\acmYear{2018}
\copyrightyear{2018}

%\acmPrice{15.00}

%\acmBadgeL[http://ctuning.org/ae/ppopp2016.html]{ae-logo}
%\acmBadgeR[http://ctuning.org/ae/ppopp2016.html]{ae-logo}


%include lhs2TeX.fmt
%include lhs2TeX.sty
%include polycode.fmt

\DeclareMathAlphabet{\mathkw}{OT1}{cmss}{bx}{n}
%subst keyword a = "\mathkw{" a "}"
%subst conid a = "\V{" a "}"
%subst varid a = "\V{" a "}"
%subst numeral a = "\C{" a "}"

\usetikzlibrary{automata}
\usetikzlibrary{shapes}
\usetikzlibrary{backgrounds}
\usetikzlibrary{positioning}

\newtheorem{Lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{Example}{Example}

\newcommand{\sembrackets}[1]{\ensuremath{\llbracket #1 \rrbracket}}

\usepackage{color}
\newcommand{\redFG}[1]{\textcolor[rgb]{0.6,0,0}{#1}}
\newcommand{\greenFG}[1]{\textcolor[rgb]{0,0.4,0}{#1}}
\newcommand{\blueFG}[1]{\textcolor[rgb]{0,0,0.8}{#1}}
\newcommand{\orangeFG}[1]{\textcolor[rgb]{0.8,0.4,0}{#1}}
\newcommand{\purpleFG}[1]{\textcolor[rgb]{0.4,0,0.4}{#1}}
\newcommand{\yellowFG}[1]{\textcolor{yellow}{#1}}
\newcommand{\brownFG}[1]{\textcolor[rgb]{0.5,0.2,0.2}{#1}}
\newcommand{\blackFG}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\whiteFG}[1]{\textcolor[rgb]{1,1,1}{#1}}
\newcommand{\yellowBG}[1]{\colorbox[rgb]{1,1,0.2}{#1}}
\newcommand{\brownBG}[1]{\colorbox[rgb]{1.0,0.7,0.4}{#1}}

\newcommand{\ColourStuff}{
  \newcommand{\red}{\redFG}
  \newcommand{\green}{\greenFG}
  \newcommand{\blue}{\blueFG}
  \newcommand{\orange}{\orangeFG}
  \newcommand{\purple}{\purpleFG}
  \newcommand{\yellow}{\yellowFG}
  \newcommand{\brown}{\brownFG}
  \newcommand{\black}{\blackFG}
  \newcommand{\white}{\whiteFG}
}

\newcommand{\MonochromeStuff}{
  \newcommand{\red}{\blackFG}
  \newcommand{\green}{\blackFG}
  \newcommand{\blue}{\blackFG}
  \newcommand{\orange}{\blackFG}
  \newcommand{\purple}{\blackFG}
  \newcommand{\yellow}{\blackFG}
  \newcommand{\brown}{\blackFG}
  \newcommand{\black}{\blackFG}
  \newcommand{\white}{\blackFG}
}

\ColourStuff

\newcommand{\D}[1]{\blue{\mathsf{#1}}}
\newcommand{\C}[1]{\red{\mathsf{#1}}}
\newcommand{\F}[1]{\green{\mathsf{#1}}}
\newcommand{\V}[1]{\black{\mathsf{#1}}}
\newcommand{\TC}[1]{\purple{\mathsf{#1}}}

%subst comment a = "\orange{\texttt{--" a "}}"

\newcommand{\conf}[1]{\ensuremath{\langle #1 \rangle}}

\begin{document}

\title{Towards certified virtual machine-based regular expression parsing}

\author{Thales Ant\^onio Delfino}
\affiliation{
  \institution{Universidade Federal de Ouro Preto}
  \city{Ouro Preto}
  \state{Minas Gerais}
  \country{Brazil}
}
\author{Rodrigo Ribeiro}
\affiliation{
  \institution{Universidade Federal de Ouro Preto}
  \city{Ouro Preto}
  \state{Minas Gerais}
  \country{Brazil}
}
\begin{abstract}
Regular expressions (REs) are pervasive in computing. We use REs in text editors, string search tools
(like GNU-Grep) and lexical analysers generators. Most of these tools rely on converting regular
expressions to its corresponding finite state machine or use REs derivatives for directly parse an
input string. In this work, we investigate the suitability of another approach: instead of
using derivatives or generate a finite state machine for a given RE, we developed a virtual machine
(VM) for parsing regular languages, in such a way that a RE is merely a program executed by the VM
over the input string. We provided a prototype implementation in Haskell, tested it using QuickCheck and
provided proof sketches of its correctness with respect to RE standard inductive semantics.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003766.10003776</concept_id>
<concept_desc>Theory of computation~Regular languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10010124.10010131.10010134</concept_id>
<concept_desc>Theory of computation~Operational semantics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Regular languages}
\ccsdesc[500]{Theory of computation~Operational semantics}
% We no longer use \terms command
%\terms{Theory}

\keywords{Regular Expressions, Parsing, Virtual Machines, Operational semantics}

\maketitle

%format Regex = "\D{Regex}"
%format Char = "\D{Char}"
%format Just = "\C{Just}"
%format Nothing = "\C{Nothing}"
%format Maybe = "\D{Maybe}"
%format String = "\D{String}"

\section{Introduction}

We name parsing the process of analyzing if a sequence of symbols matches a given set of rules.
Such rules are usually specified in a formal notation, like a grammar. If a string can be obtained
from those rules, we have success: we can build some evidence that the input is in the language
described by the underlying formalism. Otherwise, we have a failure: no such evidence exists.

In this work, we focus on the parsing problem for regular expressions (REs), which are an algebraic
and compact way of defining regular languages (RLs), i.e., languages that can be recognized by
(non-)deterministic finite automata and equivalent formalisms. REs are widely used in string search
tools, lexical analyser generators and XML schema languages \cite{Frisch2004}. Since RE parsing
is pervasive in computing, its correctness is crucial and is the subject of study of several
recent research works (e.g ~\cite{Firsov13,Ribeiro2017,Lopes2016,Asperti10}).

Approaches for RE parsing can use representations of finite state machines (e.g.~\cite{Firsov13}),
derivatives (e.g.~\cite{Ribeiro2017,Lopes2018,Lopes2016}) or the so-called pointed RE's or its
variants~\cite{Asperti10,Fischer2010}. Another approach for parsing is based on the so-called
parsing machines, which dates back to 70's with Knuth's work
on top-down syntax analysis for context-free languages~\cite{Knuth71}. Recently, some works
have tried to revive the use of such machines for parsing: Cox~\cite{Cox2009} defined a VM
for which a RE can be seen as ``high-level programs'' that can be compiled to a sequence of
such VM instructions and Lua library LPEG~\cite{Ierusalimschy2009} defines a VM whose instruction
set can be used to compile Parser Expressions Grammars (PEGs)~\cite{Ford04}. Such renewed research
interest is motivated by the fact that is possible to include new features by just adding and
implementing new machine instructions.

Since LPEG VM is designed with PEGs in mind, it is not appropriate for RE parsing, since the ``star''
operator for PEGs has a greedy semantics which differs from the conventional RE semantics for this operator. Also, Cox's work
on VM-based RE parsing has problems. First, it is poorly specified: both the VM semantics and the RE
compilation process are described only informally and no correctness guarantees is even mentioned. Second,
it does not provide an evidence for matching, which could be used to characterize a disambiguation strategy, like
Greedy~\cite{Frisch2004} and POSIX~\cite{Sulzmann14}. To the best of our knowledge, no previous work
has formally defined a VM for RE parsing that produces evidence (parse trees) for successul matches.
The objective of this work is to give a first step in filling this gap. More specifically, we are interested in formally
specify, implement and test the correctness of a VM based small-step semantics for RE parsing which produces bit-codes as
a memory efficient representation of parse-trees. As pointed by~\cite{Lasse2011}, bit-codes are useful because they
are not only smaller than the parse tree, but also smaller than the string being parsed and they can be combined with methods
for text compression. We leave the task of proving that our VM follows a specific disambiguation
strategy to future work.

Our contributions are:

\begin{itemize}
	\item We present a small-step semantics for RE inspired by  Thompson's NFA\footnote{Non-deterministic finite automata.}
	construction~\cite{Thompson1968}. The main novelty of this presentation is
	the use of data-type derivatives, a well-known concept in functional programming community, to represent
	the context in which the current RE being evaluated occur. We show informal proofs\footnote{By ``informal proofs'' we
		mean proofs that are not mechanized in a proof-assistant. Due to space reasons, proofs of the relevant theorems are omitted from this
		version. Detailed proofs can be found in the accompanying technical report avaliable on-line~\cite{regexvm-rep}.} that our semantics
	is sound and complete with respect to RE inductive semantics.
	\item We describe a prototype implementation of our semantics in Haskell and use QuickCheck~\cite{Claessen2000} to test our semantics
	against a simple implementation of RE parsing, presented in~\cite{Fischer2010}, which we prove correct in the Appendix~\ref{appendix:accept}.
	Our test cases cover both accepted and rejected strings for randomly generated REs.
	\item We show how our proposed semantics can produce bit codes that denote parse trees~\cite{Lasse2011} and test that
	such generated codes correspond to valid parsing evidence using QuickCheck.
\end{itemize}

We are aware that using automated testing is not sufficient to ensure correctness, but it can expose bugs before using more formal approaches,
like formalizing our algorithm in a proof assistant. Such semantic prototyping step is crucial since it can avoid proof attempts that are
doomed to fail due to incorrect definitions. The project's on-line repository~\cite{regexvm-rep} contains the partial Coq formalization of
our semantics. Currently, we have formalized the semantics and its interpreter function. The Coq proof that the proposed small-step semantics
is equivalent to the usual inductive RE semantics is under development.

The rest of this paper is organized as follows. Section~\ref{section:background} presents some background concepts on RE and
data type derivatives that will be used in our semantics. Our operational semantics for RE parsing and its theoretical properties
are described in Section~\ref{section:semantics}. Our prototype implementation and the QuickCheck test suit used to validate
it are presented in Section~\ref{section:implementation}. Section~\ref{section:related} discuss related work and
Section~\ref{section:conclusion} concludes.

We assume that the reader knows the Haskell programming language, specially the list monad and how it can be used
to model non-determinism.  Good introductions to Haskell are available elsewhere~\cite{Lipovaca2011}.
All source code produced, including the literate Haskell source of this
article (which can be preprocessed using lhs2\TeX~\cite{Loh2005}), instructions on how to build it and reproduce the developed
test suit are avaliable on-line~\cite{regexvm-rep}.

\section{Background}\label{section:background}

\subsection{Regular expressions: syntax and semantics}

REs are defined with respect to a given alphabet. Formally, the following context-free
grammar defines RE syntax:
\[
e ::= \emptyset\,\mid\,\epsilon\,\mid\,a\,\mid\,e\,e\,\mid\,e+e\,\mid\,e^{\star}
\]
Meta-variable $e$ will denote an arbitrary RE and $a$ an
arbitrary alphabet symbol. As usual, all meta-variables can appear primed or subscripted.
In our Haskell implementation, we represent alphabet symbols using type |Char|.

%format Epsilon = "\C{\epsilon}"
%format Empty = "\C{\emptyset}"
%format Chr = "\C{Chr}"
%format :*: = "\C{\:\bullet\:}"
%format :+: = "\C{\:+\:}"
%format Star = "\C{Star}"

\begin{spec}
data Regex = Empty | Epsilon | Chr Char | Regex :*: Regex
  | Regex :+: Regex | Star Regex
\end{spec}

Constructors |Empty| and |Epsilon| denote respectively the
empty set ($\emptyset$) and the empty string ($\epsilon$) REs. Alphabet
symbols are constructed by using the |Chr| constructor. Bigger REs are
built using concatenation (|:*:|), union (|:+:|) and
Kleene star (|Star|).

\newcommand{\Eps}{\textit{Eps}}
\newcommand{\Chr}{\textit{Chr}}
\newcommand{\Catt}{\textit{Cat}}
\newcommand{\Left}{\textit{Left}}
\newcommand{\Right}{\textit{Right}}
\newcommand{\StarBase}{\textit{StarBase}}
\newcommand{\StarRec}{\textit{StarRec}}

Following common practice~\cite{Lopes2016,Ribeiro2017,Rathnayake2011}, we adopt an inductive
characterization of RE membership semantics. We let judgment $s \in \sembrackets{e}$ denote
that string $s$ is in the language denoted by RE $e$.

\begin{figure}[h]
	\[
	\begin{array}{cc}
	\infer[_{\{\Eps\}}]
	{\epsilon \in \sembrackets{\epsilon}}{} &
	\infer[_{\{Chr\}}]
	{a \in \sembrackets{a}}{a \in \Sigma} \\ \\
	\infer[_{\{\Left\}}]
	{s \in \sembrackets{e + e'}}{s \in \sembrackets{e}} &
	\infer[_{\{\Right\}}]
	{s' \in \sembrackets{e + e'}}{s' \in \sembrackets{e'}} \\ \\
	\infer[_{\{\StarBase\}}]
	{\epsilon \in \sembrackets{e^\star}}{} &
	\infer[_{\{\StarRec\}}]
	{ss' \in \sembrackets{e^\star}}
	{s \in \sembrackets{e} & s' \in \sembrackets{e^\star}} \\ \\
	\multicolumn{2}{c}{
		\infer[_{\{\Catt\}}]
		{ss' \in \sembrackets{ee'}}
		{s \in \sembrackets{e} & s' \in \sembrackets{e'}}
	} \\
	\end{array}
	\]
	\centering
	\caption{RE inductive semantics.}
	\label{figure:resemantics}
\end{figure}

Rule $Eps$ states that the empty string (denoted by the $\epsilon$)
is in the language of RE |Epsilon|.

For any single character |a|, the singleton
string |a| is in the RL
for |Chr a|. Given membership proofs for REs
|e| and |e'|, $s \in \sembrackets{e}$ and $s' \in\sembrackets{e'}$,
rule $Cat$ can be used to build a proof
for the concatenation of these REs.  Rule
\Left~ (\Right) creates a membership proof
for $e + e'$ from a proof for $e$ ($e'$). Semantics for Kleene star
is built using the following well known equivalence of REs: $e^\star
= \epsilon + e\,e^\star$.

We say that a RE $e$ is \emph{problematic}
if $e = e'^\star$ and $\epsilon \in \sembrackets{e'}$~\cite{Frisch2004}.
In this work, we limit our attention to non-problematic RE's. Our results
can be extended to problematic REs without providing any
new insight~\cite{Lasse2011,Frisch2004}.

%format Tree  = "\D{Tree}"
%format TUnit = "\C{()}"
%format TChar = "\C{Chr}"
%format InL   = "\C{InL}"
%format InR   = "\C{InR}"
%format TList = "\C{List}"

\subsection{RE parsing and bit-coded parse trees}

\paragraph{RE parsing.}
One way to represent parsing evidence is to build a tree that denotes
a RE membership proof. Following~\cite{Lasse2011,Frisch2004}, we let
parse trees be terms whose type is underlying RE.

\begin{spec}
data Tree = TUnit | TChar Char | Tree :*: Tree | InL Tree
  | InR Tree | TList [Tree]
\end{spec}

Constructor |TUnit| denotes a tree for RE |Epsilon| and |TChar| is a tree
for a single character RE. Trees for concatenations are pairs,
constructors |InL| and |InR| denotes trees for the left and right component
of a choice operator. Finally, a tree for RE $e^\star$ is a list of trees for
RE $e$. This informal relation is specified by the following inductive
relation between parse trees and RE. We let $\vdash t : e$ denote that
$t$ is a parse tree for RE $e$.

	\begin{figure}[h]
		\[
		\begin{array}{ccc}
		\infer{\vdash |TUnit| : \epsilon}{} &
		\infer{\vdash |TChar a| : a} &
		\infer{\vdash |InL t| : e + e'}
		{\vdash |t| : e} \\ \\ \\
		\infer{\vdash |InR t'| : e + e'}
		{\vdash |t'| : e'} &
		\infer{\vdash |t :*: t'| : e e'}
		{\vdash |t| : e & \vdash |t'| : e' } &
		\infer{\vdash |TList ts| : e^\star}
		{\forall |t|. |t| \in |ts| \to \vdash |t| : e} \\
		\end{array}
		\]
		\centering
		\caption{Parse tree typing relation.}
		\label{figure:parsetreetyping}
	\end{figure}

%format flatten = "\F{flat}"
%format concatMap = "\F{concatMap}"

The relation between RE semantics and its parse trees are formalized using the
function |flatten|, which builds the string stored in a
given parse tree. The Haskell implementation of |flatten| is immediate.

\begin{spec}
flatten :: Tree -> String
flatten TUnit = ""
flatten (TChar c) = [c]
flatten (t :*: t') = flatten t ++ flatten t'
flatten (InL t) = flatten t
flatten (InR t) = flatten t
flatten (TList ts) = concatMap flatten ts
\end{spec}

\begin{Example}
\label{example:parsetree}
Consider the RE $((ab)+c)^*$ and the string $abcab$, which is accepted by that RE. Here is shown the string's corresponding parse tree:

\begin{center}
\begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
\node{$[\,]$}
child{
node{inl}
child{
node {$\langle,\rangle$}
child{node{a}}
child{node{b}}
}
}
child{
node{inr}
child{node {c}}
}
child{
node{inl}
child{
node {$\langle,\rangle$}
child{node{a}}
child{node{b}}
}
};
\end{tikzpicture}
\end{center}

\end{Example}

The next theorem, which relates parse tress and RE semantics,
can be proved by an easy induction on the RE semantics derivation.
\begin{Theorem}
   For all $s$ and $e$, if $s \in \sembrackets{e}$ then exists a tree |t|
   such that |flatten t =| $s$ and $\vdash |t| : e$.
\end{Theorem}

\begin{proof}
  We proceed by induction on the derivation of $s \in \sembrackets{e}$.
  \begin{enumerate}
    \item Case rule $(Eps)$: Then, $s = \epsilon$. Let $t = |TUnit|$ and the conclusion
          follows by the definition of |flatten| and rule $T1$.
    \item Case rule $(Chr)$: Then, $s = a$, $a \in \Sigma$. Let $t = |TChar a|$ and the conclusion
          follows by the definition of |flatten| and rule $T2$.
    \item Case rule $(Left)$: Then, $e = e_1 + e_2$ and $s \in\sembrackets{e_1}$. By the induction hypothesis,
          we have a tree $t_l$ such that |flatten tl = s| and $\vdash t_l : e_1$. Let |t = InL tl|.
          Conclusion follows from rule $T3$ and the definition of |flatten|.
    \item Case rule $(Right)$: Then, $e = e_1 + e_2$ and $s \in\sembrackets{e_2}$. By the induction hypothesis,
          we have a tree $t_r$ such that |flatten tr = s| and $\vdash t_r : e_2$. Let |t = InR tr|.
          Conclusion follows from rule $T4$ and the definition of |flatten|.
    \item Case rule $(Cat)$: Then, $e = e_1\,e_2$, $s = s_1\,s_2$, $s_1 \in \sembrackets{e_1}$ and
          $s_2\in\sembrackets{e_2}$. By the induction hypothesis we have trees $t_1$ and $t_2$ such that
          |flatten t1 = |$s_1$, $\vdash t_1 : e_1$, |flatten t2 = |$s_2$ and $\vdash t_2 : e_2$. Let |t = t1 :*: t2|.
          Conclusion follows by rule $T5$ and the definition of |flatten|.
    \item Case rule $(StarBase)$: Then, $e = e_1^\star$, $s = \epsilon$. Let |t = TList []|. Conclusion follows by rule $T6$ and
          the definition of |flatten|.
    \item Case rule $(StarRec)$: Then, $e = e_1^\star$, $s = s_1\,s_2$, $s_1 \in\sembrackets{e_2}$ and
          $s_2 \in \sembrackets{e_2}$. By induction hypothesis, we have trees $t_1$ and $t_2$ such that
          |flatten t1 = s1|, $\vdash t_1 : e_1$, |flatten t_2 = s2|, $\vdash t2 : e_1^\star$ and
          |t2 = TList ts|, for some list |ts|. Let |t = TList (t1 : ts)|. Conclusion follows from the
          definition of |flatten| and rule $T6$.
  \end{enumerate}
\end{proof}

\paragraph{Bit-coded parse trees.} Nielsen et. al.~\cite{Lasse2011} proposed the
use of bit-marks to register which branch was chosen in a parse tree for union
operator, $+$, and to delimit different matches done by Kleene star expression.
Evidently, not all bit sequences correspond to valid parse trees. Ribeiro et. al.~\cite{Ribeiro2017}
showed an inductively defined relation between valid bit-codes and RE, accordingly to the encoding
proposed by~\cite{Lasse2011}. We let the judgement $bs \rhd e$ denote that the sequence of bits
$bs$ corresponds to a parse-tree for RE $e$.

%format Bit = "\D{Bit}"
%format Zero = "\C{0_b}"
%format One  = "\C{1_b}"

\begin{figure}[h]
	\[
	\begin{array}{ccc}
	\infer{|[]| \rhd \epsilon}{} &
	\infer{|[]| \rhd a}{}  &
	\infer{|Zero : bs| \rhd e + e'}{|bs| \rhd e} \\ \\
	\infer{|One : bs| \rhd e + e'}{|bs| \rhd e'} &
	\infer{|bs ++ bs'| \rhd e e'}{|bs| \rhd e & |bs'| \rhd e'} &
	\infer{|[ One ] | \rhd e^\star}{} \\ \\
	\multicolumn{3}{c}{
		\infer{|Zero : bs ++ bss| \rhd e^\star}{|bs| \rhd e & |bss| \rhd e^\star}
	}
	\end{array}
	\]
	\centering
	\caption{Typing relation for bit-codes.}
	\label{figure:typing-bitcodes}
\end{figure}

The empty string and single character RE are both represented by empty bit lists. Codes for RE $ee'$ are
built by concatenating codes of $e$ and $e'$. In RE union operator, $+$, the bit |Zero| marks that the
parse tree for $e + e'$  is built from $e$'s and bit |One| that it is built from $e'$'s. For the Kleene
star, we use bit |One| to denote the parse tree for the empty string and bit |Zero| to begin matchings of $e$
in a parse tree for $e^\star$.

%format code = "\F{code}"
%format decode' = "\F{dec}"
%format decode = "\F{decode}"
%format Code = "\D{Code}"
%format codeList = "\F{codeList}"
%format fail = "\F{fail}"
%format return = "\F{return}"
%format decodeList = "\F{decodeList}"
%format foldr = "\F{foldr}"

The relation between a bit-code and its underlying parse tree can be defined using functions
|code| and |decode|. Type |Code| used in |code| and |decode| definition is just a synonym
for |[Bit]|. Function |code| has an immediate definition by recursion on the structure of parse tree.

\begin{spec}
code :: Tree -> Regex -> Code
code (InL t) (e :+: _) = Zero : code t e
code (InR t') (_ :+: e') = One : code t' e'
code (TList ts) (Star e) = codeList ts e
code (t :*: t') (e :*: e') = code t e ++ code t' e'
code _ _ = []

codeList :: [Tree] -> Regex -> Code
codeList ts e = foldr (\ t ac -> Zero : code t e ++ ac) [One] ts
\end{spec}

To define function |decode|, we need to keep track of the remaining bits to be processed to finish
tree construction. This task is done by an auxiliar definition, |decode'|.

\begin{spec}
decode' :: Regex -> Code -> Maybe (Tree, Code)
decode' Epsilon bs = return (TUnit, bs)
decode' (Chr c) bs = return (TChar c, bs)
decode' (e :+: _) (Zero : bs) = do
     (t,bs1) <- decode' e bs
     return (InL t, bs1)
decode' (_ :+: e') (One : bs) = do
     (t',bs1) <- decode' e' bs
     return (InR t', bs1)
decode' (e :*: e') bs = do
      (t, bs1) <- decode' e bs
      (t', bs') <- decode' e' bs1
      return (t :*: t', bs')
decode' (Star e) bs = do
      (ts,bs') <- decodeList e bs
      return (TList ts, bs')
decode' _ _ = fail "invalid bit code"
\end{spec}
For single character and empty string REs, its decoding consists in just building
the tree and leaving the input bit-coded untouched. We build a left tree (using |InL|)
for $e + e'$ if the code starts with bit |Zero|. A parse tree using constructor |InR| is built
whenever we find bit |One| for a union RE. Building a tree for concatenation is done by
sequencing the processing of codes for left component of concatenation and starting the
processing of right component with the remaining bits from the processing of the left RE.
\begin{spec}
decodeList :: Regex -> Code -> Maybe ([Tree], Code)
decodeList _ [] = fail "fail decodeList"
decodeList _ (One : bs) = return ([], bs)
decodeList e (Zero : bs) = do
      (t, be) <- decode' e bs
      (ts, bs') <- decodeList e be
      return (t : ts, bs')
\end{spec}
Function |decodeList| generate a list of parse trees consuming the bit |Zero| used
as a separator, and bit |One| which finish the list of parsing results for star operator.

Finally, using |dec|, the definition of |decode| is immediate.
\begin{spec}
decode :: Regex -> Code -> Maybe Tree
decode e bs
  = case decode' e bs of
       Just (t, []) -> Just t
       _            -> Nothing
\end{spec}

\begin{Example}
We present again the same RE and string we showed in Example \ref{example:parsetree}, denoted by $((ab) + c)^*$ and $abcab$, respectively. Note that the parse tree is also the same. However, this time it contains its bit codes, which are $0001001$. The first, third and fifth zeros in this sequence are separators and do not appear on the tree, as well as the last one digit, which defines the end of the bit codes. Remaining three digits (two zeros and one one) appear in each $inl$ or $inr$ on the tree.

\begin{center}
\begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
\node{$[\,]$}
child{
node{0:inl}
child{
node {$\langle,\rangle$}
child{node{a}}
child{node{b}}
}
}
child{
node{1:inr}
child{node {c}}
}
child{
node{0:inl}
child{
node {$\langle,\rangle$}
child{node{a}}
child{node{b}}
}
};
\end{tikzpicture}
\end{center}

\end{Example}

The relation between codes and its correspondent parse trees are specified by the next
theorem.
\begin{Theorem}
  Let $t$ be a parse tree such that $\vdash t : e$, for some RE e. Then $|(code t e)| \rhd e$ and
  |decode e (code t e) = Just t|.
\end{Theorem}
\begin{proof}
  We proceed by induction on the derivation of $\vdash t : e$.
  \begin{enumerate}
    \item Case rule $T1$: Then, $e = \epsilon$ and |t = TUnit|. Conclusion follows by rule $B1$ and
          the definition of functions |code| and |decode|.
    \item Case rule $T2$: Then, $e = a$ and |t = TChar a|. Conclusion follows by rule $B2$ and
          the definition of functions |code| and |decode|.
    \item Case rule $T3$: Then, $e = e1 + e2$ and |t = InL tl|. By the induction hypothesis,
          we have that |code tl e1 = bs|,  $bs \rhd e1$ and |decode e1 bs = Just tl|. From the
          definition of |code|, we have that |code (InL tl) (e1 + e2) = Zero : bs| and by rule $B3$,
          we have that $|Zero| : bs \rhd (e1 + e2)$. The conclusion follows from the definition of |code|,
          |decode| and the fact that |decode e1 bs = Just tl|.
    \item Case rule $T4$: Then, $e = e1 + e2$ and |t = InR tr|. By the induction hypothesis,
          we have that |code tr e2 = bs|,  $bs \rhd e2$ and |decode e2 bs = Just tr|. From the
          definition of |code|, we have that |code (InR tl) (e1 + e2) = One : bs| and by rule $B4$,
          we have that $|One| : bs \rhd (e1 + e2)$. The conclusion follows from the definition of |code|,
          |decode| and the fact that |decode e2 bs = Just tr|.
    \item Case rule $T5$: Then, $e = e_1\,e_2$ and |t = tl :*: tr|. Conclusion follows from the induction
          hypothesis on |tl| and |tr|.
    \item Case rule $T6$: Then, $e = e_1^\star$ and |t = TList ts|,
          where $\forall t'. t' \in ts \to \vdash t' : e_1$. The desired conclusion follows from the induction hypothesis
          on each tree $t' \in ts$.
  \end{enumerate}
\end{proof}
Next, we review Thompson NFA construction which is similar to the proposed semantics
for RE parsing developed in Section~\ref{section:semantics}.

\subsection{Thompson NFA construction}
\label{subsection:thompsonnfraconstruction}

The Thompson NFA construction is a classical algorithm for building an equivalent
NFA with $\epsilon$-transitions by induction over the structure of an input RE.
We follow a presentation given in~\cite{Aho1986} where $N(e)$ denotes the NFA
equivalent to RE $e$. The construction proceeds as follows. If $e = \epsilon$,
we can build the following NFA equivalent to $e$.
\begin{center}
  \begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
      \node[state, initial, fill=white]   (q_1) [] {};
      \node[state, accepting, fill=white] (q_2) [right of=q_1] {};

      \path[->] (q_1) edge [] node {$\epsilon$}  (q_2);
  \end{tikzpicture}
\end{center}
If $e = a$, for $a \in \Sigma$, we can make a NFA with a single transition consuming
$a$:
\begin{center}
 \begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
      \node[state, initial, fill=white]   (q_1) [] {};
      \node[state, accepting, fill=white] (q_2) [right of=q_1] {};

      \path[->] (q_1) edge [] node {$a$}  (q_2);
 \end{tikzpicture}
\end{center}
When $e = e_1 + e_2$, we let $N(e_1)$ be the NFA for $e_1$ and $N(e_2)$ the
NFA for $e_2$. The NFA for $e_1 + e_2$ is built by adding a new initial and accepting
state which can be combined with $N(e_1)$ and $N(e_2)$ using $\epsilon$-transitions as
shown in the next picture.
\begin{center}
    \begin{tikzpicture}[auto, node distance=17mm, initial text=, >=latex]
      \node[state, initial]  (s_i)   []                   {};
      \node[state]        (a_1)   [above right of=s_i] {};
      \node[draw=none,fill=none]            (namea) [right of=a_1] {$N(e_1)$};
      \node[state]         (a_2)   [right of=namea]     {};

      \node[state]        (b_1)   [below right of=s_i] {};
      \node[draw=none]            (nameb) [right of=b_1]           {$N(e_2)$};
      \node[state]         (b_2)   [right of=nameb]     {};

      \node[state, accepting] (s_a)   [below right of=a_2] {};

      \path[->] (s_i) edge [below right] node {$\epsilon$} (a_1)
                      edge [above right] node {$\epsilon$} (b_1)
                (a_2) edge [below left]  node {$\epsilon$} (s_a)
                (b_2) edge []            node {$\epsilon$} (s_a);
      \begin{scope}[on background layer]
        \node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=b_1] {};
        \node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=a_1] {};
      \end{scope}
    \end{tikzpicture}
\end{center}
The NFA for the concatenation $e = e_1e_2$ is built from the NFAs $N(e_1)$ and $N(e_2)$. The accepting
state of $N(e_1e_2)$ will be the accepting state from $N(e_2)$ and the starting state of $N(e_1)$ will be
the initial state of $N(e_1)$.

\begin{center}
    \begin{tikzpicture}[auto,  node distance=15mm, initial text=, >=latex]
      \node[state, initial]   (a_1)   []               {};
      \node[draw=none,fill=none]             (namea) [right of=a_1]   {$N(e_1)$};
      \node[state] (a_2)   [right of=namea] {};

      \node[draw=none]             (nameb) [right of=a_2]   {$N(e_2)$};
      \node[state, accepting]  (b_2)   [right of=nameb] {};

      \begin{scope}[on background layer]
        \node[ellipse, draw=black, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_1] {};
        \node[ellipse, draw=black, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_2] {};
      \end{scope}
    \end{tikzpicture}
\end{center}
Finally, for the Kleene star operator, we built a NFA for the RE $e$, add a new
starting and accepting states and the necessary $\epsilon$ transitions, as shown below.
\begin{center}
    \begin{tikzpicture}[auto, node distance=15mm, initial text=, >=latex]
      \node[state, initial]  (s_i)   []               {};
      \node[state]        (a_1)   [right of=s_i]   {};
      \node[draw=none]            (namea) [right of=a_1]   {$N(e_1)$};
      \node[state]         (a_2)   [right of=namea] {};

      \node[state, accepting] (s_a)   [right of=a_2]   {};

      \path[->] (s_i) edge []                     node {$\epsilon$} (a_1)
                      edge [bend right=40, below] node {$\epsilon$} (s_a)
                (a_2) edge []                     node {$\epsilon$} (s_a)
                      edge [bend right=90, above] node {$\epsilon$} (a_1);
      \begin{scope}[on background layer]
        \node[ellipse, draw=black, aspect=5, minimum width=50mm, minimum height=20mm, right of=a_1] {};
      \end{scope}
    \end{tikzpicture}
\end{center}

\begin{Example}
    In order to show a step-by-step automata construction following Thompson's algorithm, we take as
    example the RE $((ab) + c)^* $ over the alphabet $\Sigma = \{a,b,c\}$.

    The first step is to construct an automata ($S_1$) that accepts the symbol $a$.

   \begin{center}
	\begin{tikzpicture}[->,>=stealth',initial text =$S_1:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]

	\node[state,initial] (1) {$1$};
	\node[state,accepting] (2) [right of=1] {$2$};

	\path (1) edge              node {$a$} (2);

	\end{tikzpicture}
   \end{center}

   Then, we construct another automata ($S_2$) that accepts the symbol $b$:

   \begin{center}
	\begin{tikzpicture}[->,>=stealth',initial text =$S_2:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]

	\node[state,initial] (3) {$3$};
	\node[state,accepting] (4) [right of=3] {$4$};

	\path (3) edge              node {$b$} (4);

	\end{tikzpicture}
   \end{center}

   The concatenation $ab$ is accepted by automata $S_3$:

   \begin{center}
	\begin{tikzpicture}[->,>=stealth',initial text =$S_3:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]

	\node[state,initial] (1) {$1$};
	\node[state] (2) [right of=1] {$2$};
	\node[state,accepting] (4) [right of=2] {$4$};

	\path (1) edge              node {$a$} (2)
	(2) edge              node {$b$} (4);

	\end{tikzpicture}
  \end{center}

  Now we build automata $S_4$, which recognizes the symbol $c$:

  \begin{center}
	\begin{tikzpicture}[->,>=stealth',initial text =$S_4:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]

	\node[state,initial] (5) {$5$};
	\node[state,accepting] (6) [right of=5] {$6$};

	\path (5) edge              node {$c$} (6);

	\end{tikzpicture}
  \end{center}

  The automata $S_5$ accepts the RE $(ab) + c$:

  \begin{center}
	\begin{tikzpicture}[->,>=stealth',initial text =$S_5:$,shorten >=1pt,auto,node distance=15mm,scale = 1,transform shape]

	\node[state,initial] (7) {$7$};
	\node[state] (1) [above right of=7] {$1$};
	\node[state] (5) [below right of=7] {$5$};
	\node[state] (2) [below right of=1] {$2$};
	\node[state] (4) [above right of=2] {$4$};
	\node[state,accepting] (8) [below right of=4] {$8$};
	\node[state] (6) [right of=5] {$6$};

	\path (7) edge              node {$\epsilon$} (1)
	(7) edge              node {$\epsilon$} (5)
	(1) edge              node {$a$} (2)
	(2) edge              node {$b$} (4)
	(4) edge              node {$\epsilon$} (8)
	(5) edge              node {$c$} (6)
	(6) edge              node {$\epsilon$} (8);


	\end{tikzpicture}
  \end{center}

  Finally, we have the NFA $S_6$, that accepts $((ab) + c)^*$:

  \begin{center}
	\begin{tikzpicture}[->,>=stealth',initial text =$S_6:$,shorten >=1pt,auto,node distance=13mm,scale = 1,transform shape]

	\node[state,initial] (9) {$9$};
	\node[state] (7) [right of=9] {$7$};
	\node[state] (1) [above right of=7] {$1$};
	\node[state] (5) [below right of=7] {$5$};
	\node[state] (2) [below right of=1] {$2$};
	\node[state] (4) [above right of=2] {$4$};
	\node[state] (8) [below right of=4] {$8$};
	\node[state] (6) [right of=5] {$6$};

	\node[state,accepting] (10) [right of=8] {$10$};

	\path (7) edge              node {$\epsilon$} (1)
	(7) edge              node {$\epsilon$} (5)
	(1) edge              node {$a$} (2)
	(2) edge              node {$b$} (4)
	(4) edge              node {$\epsilon$} (8)
	(5) edge              node {$c$} (6)
	(6) edge              node {$\epsilon$} (8)
	(9) edge [bend right=60]             node [above] {$\epsilon$} (10)
	(8) edge  [bend right=100]            node {$\epsilon$} (7)
	(9) edge node {$\epsilon$} (7)
	(8) edge node {$\epsilon$} (10);

	\end{tikzpicture}
   \end{center}
\end{Example}

Originally, Thompson formulate its construction as a IBM 7094 program~\cite{Thompson1968}. Next we reformulate it as a small-step
operational semantics using contexts, modeled as data-type derivatives for RE, which is the subject of
the next section.

\subsection{Data-type derivatives}

The usage of evaluation contexts is standard in reduction semantics~\cite{Felleisen2009}.
Contexts for evaluating a RE during the parse of a string $s$ can be defined by the following
context-free syntax:
\[E[\,] \to E[\,]+ e\,\mid\,e + E[\,]\,\mid\,E[\,]\,e\,\mid\,e\,E[\,]\,\mid\,\star\]

The semantics of a $E[\,]$ context is a RE with a hole that needs to be ``filled'' to form a
RE. We have two cases for union and concatenation denoting that the hole could be the left
or the right component of such operators. Since the Kleene star has only a recursive occurrence,
it is denoted just as a ``mark'' in context syntax.

Having defined our semantics (Figure~\ref{figure:smallstep}), we have noticed that our RE context syntax is exactly the data type
for \emph{one-hole contexts}, known as derivative of an algebraic data type.
Derivatives where introduced by McBride and its coworkers~\cite{McBride08} as a generalization
of Huet's zippers for a large class of algebraic data types~\cite{AbbottAGM03}. RE contexts are
implemented by the following Haskell data-type:
%format Ctx = "\D{Hole}"
%format InChoiceL = "\C{InChoiceL}"
%format InChoiceR = "\C{InChoiceR}"
%format InCatL    = "\C{InCatL}"
%format InCatR    = "\C{InCatR}"
%format InStar    = "\C{InStar}"
\begin{spec}
data Ctx = InChoiceL Regex | InChoiceR Regex
  | InCatL Regex | InCatR Regex | InStar
\end{spec}
Constructor |InChoiceL| store the right component of a union RE (similarly for |InChoiceR|). We need
to store contexts for union because such information is used to allow backtracking in case of failure.
Constructors |InCatL| and |InCatR| store the right (left) component of a concatenation and they are
used to store the next subexpresssions that need to be evaluated during input string parsing.
Finally, |InStar| marks that we are currently processing an expression with a Kleene star operator.

\section{Proposed semantics}\label{section:semantics}

%format Type = "\D{Dir}"
%format Begin = "\C{Begin}"
%format End = "\C{End}"

In this section we present the definition of an operational semantics for RE parsing which is
equivalent to executing the Thompson's construction NFA over the input string. Observe that
the inductive semantics for RE (Figure~\ref{figure:resemantics}) can be understood as a big-step
operational semantics for RE, since it ignores many details on how should we proceed to match
an input~\cite{Rathnayake2011}.

The semantics is defined as a binary relation between \emph{configurations}, which are 5-uples
$\conf{d,e,c,b,s}$ where:
\begin{itemize}
  \item $d$ is a direction, which specifies if the semantics is starting (denoted by $B$) or
        finishing ($F$) the processing of the current expression $e$.
  \item $e$ is the current expression being evaluated;
  \item $c$ is a context in which $e$ occurs. Contexts are just a list of
        |Ctx| type in our implementation.
  \item $b$ is a bit-code for the current parsing result, in reverse order.
  \item $s$ is the input string currently being processed.
\end{itemize}
Notation $\conf{d,e,c,b,s}\to\conf{d',e',c',b',s'}$ denotes that from
configuration $\conf{d,e,c,b,s}$ we can give a step leading to a new state
$\conf{d',e',c',b',s'}$ using the rules specified in Figure~\ref{figure:smallstep}.

\begin{figure*}[h]
  \[
     \begin{array}{ccc}
        \infer[_{(Eps)}]{\conf{B,\epsilon,c,b,s} \to \conf{F,\epsilon,c,b,s}}{}
        &
        \infer[_{(Chr)}]{\conf{B,a,c,b,a:s} \to \conf{F,a,c,b,s}}{}
        &
        \infer[_{(Left_B)}]{\conf{B,e+e',c,b,s}\to\conf{B,e,c',b',s}}
              {\begin{array}{c}
                 b' = |Zero| : b\\
                 c' = E[\,]+e' : c \\
               \end{array}}
        \\ \\
        \infer[_{(Right_B)}]{\conf{B,e+e',c,b,s}\to\conf{B,e',c',b',s}}
              {\begin{array}{c}
                 b' = |One| : b\\
                 c' = e + E[\,] : c \\
               \end{array}}
        &
        \infer[_{(Cat_B)}]{\conf{B,ee',c,b,s}\to\conf{B,e,c',b,s}}
              {c' = E[\,]e' : c}
        &
        \infer[_{(Star_1)}]{\conf{B,e^\star,c,b,s}\to\conf{B,e,\star : c, |Zero| : b, s}}{}
        \\ \\
        \infer[_{(Star_2)}]{\conf{B,e^\star,c,b,s}\to\conf{F,e^\star, c, |One| : b, s}}{}
        &
        \infer[_{(Cat_{EL})}]{\conf{F,e,E[\,]e':c,b,s}\to\conf{B,e',c',b,s}}{c'=eE[\,]:c}
        &
        \infer[_{(Cat_{ER})}]{\conf{F,e',eE[\,]:c,b,s}\to\conf{F,ee',c,b,s}}{}
        \\ \\
        \multicolumn{3}{c}{
            \begin{array}{cc}
               \infer[_{(Left_E)}]{\conf{F,e,c,b,s}\to\conf{F,e+e',c',|Zero|:b,s}}{c = E[\,]+e' : c'}
            &
               \infer[_{(Right_E)}]{\conf{F,e,c,b,s}\to\conf{F,e+e',c',|One|:b,s}}{c = e + E[\,] : c'}
            \end{array}}
        \\ \\
        \multicolumn{3}{c}{
           \begin{array}{cc}
              \infer[_{(Star_{E1})}]{\conf{F,e,\star:c,b,s}\to\conf{B,e,\star:c,|Zero|:b,s}}{}
              &
              \infer[_{(Star_{E2})}]{\conf{F,e,\star:c,b,s}\to\conf{F,e^\star,c,|One|:b,s}}{}
           \end{array}
        }
     \end{array}
  \]
  \centering
  \caption{Small-step semantics for RE parsing.}
  \label{figure:smallstep}
\end{figure*}
The rules of the semantics can be divided in two groups: starting rules and finishing rules.
Starting rules deal with configurations with a begin ($B$) direction and denote that we are
beginning the parsing for its RE $e$. Finishing rules use the context to decide how the parsing
for some expression should end. Intuitively, starting rules correspond to transitions entering a
sub-automata of Thompson NFA and finishing rules to transitions exiting a sub-automata.

The meaning of each starting rule is as follows. Rule $\{Eps\}$ specifies that we can mark a state as
finished if it consists of a starting configuration with RE $\epsilon$. We can finish any configuration
for RE |Chr a| if it is starting with current string with a leading $a$. Whenever we have a starting configuration
with a choice RE, $e_1 + e_2$, we can non-deterministically choose if input string $s$ can be processed by
$e_1$ (rule $Left_B$) or $e_2$ (rule $Right_B$). For beginning configurations with concatenation, we parse
input string using each of its components sequentially. Finally, for starting configurations with a Kleene
star operator, $e^\star$, we can either start the processing of $e$ or finish the processing for $e^\star$.
In all recursive cases for RE, we insert context information in the third component of the resulting
configuration in order to decide how the machine should step after finishing the execution of the RE
currently on focus.

Rule $(Cat_{EL})$ applies to any configuration which is finishing with a left concatenation context ($E[\,]e'$).
In such situation, rule specifies that a computation should continue with $e'$ and push the context $e\,E[\,]$.
We end the computation for a concatenation, whenever we find a context $e\,E[\,]$ in the context component
(rule $(Cat_{ER})$). Finishing a computation for choice consists in just popping its correspondent context,
as done by rules $(Left_E)$ and $(Right_E)$. For the Kleene star operator, we can either finish the computation
by popping the contexts and adding the corresponding |One| to end its matching list or restart with RE $e$ for
another matching over the input string.

The proposed semantics is inspired by Thompson's NFA construction
(as shown in Section \ref{subsection:thompsonnfraconstruction}).
First, the rule $Eps$ can be understood as executing the transition highlighted
in red in the following schematic automata.

\begin{center}
\begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
\node[state, initial, fill=white]   (q_1) [] {};
\node[state, accepting, fill=white] (q_2) [right of=q_1] {};

\path[->](q_1) edge [red] node {$\epsilon$}  (q_2);
\end{tikzpicture}
\end{center}

The $Chr$ rule corresponds to the following transition (represented in red) in the next automata.

\begin{center}
\begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
\node[state, initial, fill=white]   (q_1) [] {};
\node[state, accepting, fill=white] (q_2) [right of=q_1] {};

\path[->] (q_1) edge [red] node {$a$}  (q_2);
\end{tikzpicture}
\end{center}

Rule $Cat_B$ corresponds to start the processing of the input string in the automata $N(e_1)$;
while rule $Cat_{EL}$ deals with exiting the automata  $N(e_1)$ followed by processing the remaining
string in $N(e_2)$. Rule $Cat_{ER}$ deals with ending the processing in the automata below.

\begin{center}
\begin{tikzpicture}[auto,  node distance=15mm, initial text=, >=latex]
\node[state, initial]   (a_1)   []               {};
\node[draw=none,fill=none]             (namea) [right of=a_1] {$N(e_1)$};
\node[state] (a_2)   [right of=namea] {};

\node[draw=none]             (nameb) [right of=a_2]  {$N(e_2)$};
\node[state, accepting]  (b_2)   [right of=nameb] {};

\begin{scope}[on background layer]
\node[ellipse, draw=red, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_1] {};
\node[ellipse, draw=red, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_2] {};
\end{scope}
\end{tikzpicture}
\end{center}

If we consider a RE $e = e_1 + e_2$ and lets $N(e_1)$ and $N(e_2)$ be two NFAs for
$e_1$ and $e_2$, respectively, we have the following correspondence between transtions and
semantics rules in the next NFA:

\begin{itemize}
\item Red transition for rule $Left_B$;
\item Green for $Right_B$;
\item Blue for $Left_E$; and
\item Black for $Right_E$.
\end{itemize}

\begin{center}
\begin{tikzpicture}[auto, node distance=17mm, initial text=, >=latex]
\node[state, initial]  (s_i)   []                   {};
\node[state]        (a_1)   [above right of=s_i] {};
\node[draw=none,fill=none]            (namea) [right of=a_1] {$N(e_1)$};
\node[state]         (a_2)   [right of=namea]     {};

\node[state]        (b_1)   [below right of=s_i] {};
\node[draw=none]            (nameb) [right of=b_1]           {$N(e_2)$};
\node[state]         (b_2)   [right of=nameb]     {};

\node[state, accepting] (s_a)   [below right of=a_2] {};

\path[->] (s_i) edge [below right, red] node {$\epsilon$} (a_1)
edge [above right, green] node {$\epsilon$} (b_1)
(a_2) edge [below left, blue]  node {$\epsilon$} (s_a)
(b_2) edge []            node {$\epsilon$} (s_a);
\begin{scope}[on background layer]
\node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=b_1] {};
\node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=a_1] {};
\end{scope}
\end{tikzpicture}
\end{center}

Finally, we present Kleene star rules in next automata according to Thompson's NFA construction.
The colors are red for $Star_1$ rule, green for $Star_2$, blue for  $Star_{E1}$ and black for $Star_{E2}$.

\begin{center}
\begin{tikzpicture}[auto, node distance=15mm, initial text=, >=latex]
\node[state, initial]  (s_i)   []               {};
\node[state]        (a_1)   [right of=s_i]   {};
\node[draw=none]            (namea) [right of=a_1]   {$N(e_1)$};
\node[state]         (a_2)   [right of=namea] {};

\node[state, accepting] (s_a)   [right of=a_2]   {};

\path[->] (s_i) edge [red]                     node {$\epsilon$} (a_1)
edge [bend right=40, below, green] node {$\epsilon$} (s_a)
(a_2) edge [black]                     node {$\epsilon$} (s_a)
edge [bend right=90, above, blue] node {$\epsilon$} (a_1);
\begin{scope}[on background layer]
\node[ellipse, draw=black, aspect=5, minimum width=50mm, minimum height=20mm, right of=a_1] {};
\end{scope}
\end{tikzpicture}
\end{center}

The starting state of the semantics is given by the configuration
$\conf{B,e,[],[],s}$ and accepting configurations are $\conf{F,e',[],bs,[]}$, for some RE $e'$ and code $bs$.
Following common practice, we let $\to^\star$ denote the reflexive, transitive closure of the small-step
semantics defined in Figure~\ref{figure:smallstep}.
We say that a string $s$ is accepted by RE $e$ if $\conf{B,e,[],[],s}\to^\star\conf{F,e',[],bs,[]}$.
The next theorem asserts that our semantics is sound and complete with respect to RE
inductive semantics (Figure~\ref{figure:resemantics}).

\begin{Theorem}
   For all strings $s$ and non-problematic REs $e$, $s\in\sembrackets{e}$ if, and only if, $\conf{B,e,[],[],s}\to^\star\conf{F,e',[],b,[]}$ and
   $\conf{F,e',[],b,[]}$ is an accepting configuration.
\end{Theorem}
\begin{proof}
   $(\to)$: We proceed by induction on the derivation of $s\in\sembrackets{e}$.
     \begin{enumerate}
        \item Case rule $Eps$: Then, $e = \epsilon$, $s = \epsilon$ and the conclusion is immediate.
        \item Case rule $Chr$: Then, $e = a$, $s = a$ and the conclusion follows.
        \item Case rule $Left$: Then, $e = e_1 + e_2$ and $s \in\sembrackets{e_1}$. By the induction hypothesis,
              we have $\conf{B,e_1,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and the conclusion follows.
        \item Case rule $Right$: Then, $e = e_1 + e_2$ and $s \in\sembrackets{e_2}$. By the induction hypothesis,
              we have $\conf{B,e_2,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and the conclusion follows.
        \item Case rule $Cat$: Then, $e = e_1\:e_2$, $s_1 \in\sembrackets{e_1}$, $s_2\in\sembrackets{e_2}$ and
              $s = s_1\,s_2$. By the induction hypothesis on $s_1 \in\sembrackets{e_1}$ we have that
              $\conf{B,e_1,ctx,b,s}\to^\star\conf{E,e',E[\,]\,e_2 : ctx,b',[]}$ and by
              induction hypothesis on $s_2\in\sembrackets{e_2}$, we have
              $\conf{B,e_2,e_1\,E[\,]:ctx,b,s}\to^\star\conf{E,e',ctx,b',[]}$ and the conclusion follows.
        \item Case rule $StarBase$: Then, $e = e_1^\star$ and $s = \epsilon$. The conclusion is immediate.
        \item Case rule $StarRec$: Then, $e = e_1^\star$, $s = s_1s_2$, $s_1 \in \sembrackets{e_1}$ and
              $s_2\in\sembrackets{e_1^\star}$. By the induction hypothesis on  $s_1 \in \sembrackets{e_1}$, we
              have $\conf{B,e_1,\star : ctx,b,s_1}\to^\star\conf{E,e', \star: ctx,b',[]}$, the induction
              hypothesis on $s_2\in\sembrackets{e_1^\star}$ give us $\conf{B,e_1^\star,\star : ctx,b,s_2}\to^\star\conf{E,e', \star: ctx,b',[]}$
              and conclusion follows.
     \end{enumerate}
  $(\leftarrow)$: We proceed by induction on $e$.
  \begin{enumerate}
     \item Case $e = \epsilon$. Then, we have
      $\conf{B,\epsilon,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and $s = \epsilon$. Conclusion follows by rule $Eps$.
     \item Case $e = a$. Then
      $\conf{B,a,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and $s = a$. Conclusion follows by rule $Chr$.
     \item Case $e = e_1 + e_2$. Now, we consider the following cases.
     \begin{enumerate}
        \item $s$ is accepted by $e_1$. Then, we have the following derivation:
            \[\conf{B,e_1 + e_2,ctx,b,s}\to\conf{B,e_1,E[\,] + e_2 : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}\]
        By induction hypothesis on $e_1$ and the derivation $\conf{B,e_1,E[\,] + e_2 : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}$
        we have $s \in\sembrackets{e_1}$ and the conclusion follows by rule $Left$.
        \item $s$ is accepted by $e_2$. Then, we have the following derivation:
            \[\conf{B,e_2,ctx,b,s}\to\conf{B,e_1,e_1 + E[\,] : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}\]
        By induction hypothesis on $e_2$ and the derivation $\conf{B,e_1,e_1 + E[\,] : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}$,
        we have $s \in\sembrackets{e_2}$ and conclusion follows by rule $Right$.
     \end{enumerate}
  \end{enumerate}
\end{proof}


\section{Implementation details}\label{section:implementation}

%format Conf = "\D{Conf}"
%format finish = "\F{finish}"
%format nullable = "\F{nullable}"
%format Bool = "\D{Bool}"
%format True = "\C{True}"
%format False = "\C{False}"

In order to implement the small-step semantics of Figure~\ref{figure:smallstep}, we need to represent configurations.
We use type |Conf| to denote configurations and directions are represented by type |Type|, where |Begin| denote the
starting and |End| the finishing direction.

\begin{spec}
data Type = Begin | End
type Conf = (Type, Regex, [Ctx], Code, String)
\end{spec}

Function |finish| tests if a configuration is an accepting one.

\begin{spec}
finish :: Conf -> Bool
finish (End, _, [], _, []) = True
finish _ = False
\end{spec}

%format next = "\F{next}"
%format otherwise = "\F{otherwise}"

The small-step semantics is implemented by function |next|, which returns a list of configurations that can
be reached from a given input configuration. We will begin by explaining the equations that code the set of
starting rules from the small-step semantics. The first alternative

\begin{spec}
next :: Conf -> [Conf]
next (Begin, Epsilon, ctx, bs,s) = [(End, Epsilon, ctx, bs,s)]
\end{spec}
implements rule $(Eps)$, which finishes a starting |Conf| with an |Epsilon|. Rule $(Chr)$ is implemented by
the following equation
\begin{spec}
next (Begin, Chr c, ctx, bs, a:s)
  | a == c =  [(End, Chr c, ctx, bs, s)]
  | otherwise = []
\end{spec}
which consumes input character |a| if it matches RE |Chr c|, otherwise it fails by returning an empty list.
For a choice expression, we can use two distinct rules: one for parsing the input using its left component and
another rule for the right. Since both union and Kleene star introduce non-determinism in RE parsing, we can
easily model this using the list monad, by return a list of possible resulting configurations.
\begin{spec}
next (Begin, e :+: e', ctx, bs, s)
  = [ (Begin, e, InChoiceL e' : ctx, Zero : bs, s)
    , (Begin, e', InChoiceR e : ctx, One : bs, s) ]
\end{spec}
Concatenation just sequences the computation of each of its composing RE.
\begin{spec}
next (Begin, e :*: e', ctx, bs, s)
  = [(Begin, e, InCatL e' : ctx, bs, s)]
\end{spec}
For a starting configuration with Kleene star operator, |Star e|, we can proceed in two ways: by beginning the
parsing of RE |e| or by finishing the computation for |Star e| over the input.
\begin{spec}
next (Begin, Star e, ctx, bs, s)
  = [ (Begin, e , InStar : ctx, Zero : bs, s)
    , (End, (Star e), ctx, One : bs, s) ]
\end{spec}
The remaining equations of |next| deal with operational semantics finishing rules. The equation below implements
rule $(Cat_{EL})$ which specifies that an ended computation for the left component of a concatenation should continue
with its right component.
\begin{spec}
next (End, e, InCatL e' : ctx, bs, s)
  = [(Begin, e', InCatR e : ctx, bs, s)]
\end{spec}
Whenever we are in a finishing configuration with a right concatenation context, (|InCatR e|), we end the parsing of
the input for the whole concatenation RE.
\begin{spec}
next (End, e', InCatR e : ctx, bs, s)
  = [(End, e :*: e', ctx, bs, s)]
\end{spec}
Next equations implement the rules that finish configurations for the union, by committing to its first successful branch.
\begin{spec}
next (End, e, InChoiceL e' : ctx, bs, s)
  = [(End, e :+: e', ctx, Zero : bs, s)]
next (End, e', InChoiceR e : ctx, bs, s)
  = [(End, e :+: e', ctx, One : bs, s)]
\end{spec}
Equations for Kleene star implement rules $(Star_{E1})$ and $(Star_{E2})$ which allows ending or add one more match
for an RE $e$.
\begin{spec}
next (End, e, InStar : ctx, bs, s)
  = [ (Begin, e, InStar : ctx, Zero : bs, s)
    , (End, (Star e), ctx, One : bs, s) ]
\end{spec}
Finally, stuck states on the semantics are properly handled by the following equation which turns them all
into a failure (empty list).
\begin{spec}
next _ = []
\end{spec}
%format steps = "\F{steps}"
%format taccept = "\F{vmAccept}"
%format bitcode = "\F{bitcode}"
%format init_cfg = "\F{init_{cfg}}"
%format head = "\F{head}"
%format null = "\F{null}"
%format reverse = "\F{reverse}"
The reflexive-transitive closure of the semantics is implemented by function |steps|, which computes the
trace of all states needed to determine if a string can be parsed by the RE $e$.
\begin{spec}
steps :: [Conf] -> [Conf]
steps [] = []
steps cs = steps [c' | c <- cs, c' <- next c] ++ cs
\end{spec}
Finally, the function for parsing a string using an input RE is implemented as follow
s:
\begin{spec}
taccept :: String -> Regex -> (Bool, Code)
taccept s e = let r = [c | c <- steps init_cfg, finish c]
              in if null r then (False, []) else (True, bitcode (head r))
              where
                init_cfg = [(Begin, e, [], [], s)]
                bitcode (_,_,_,bs,_) = reverse bs
\end{spec}
Function |taccept| returns a pair formed by a boolean and the bit-code produced during the parsing of
an input string and RE. Observe that we need to reverse the bit-codes, since they are built in reverse
order.

\subsection{Test suite}

\paragraph{An overview of QuickCheck.}
Our tests are implemented using QuickCheck~\cite{Claessen2000}, a library
that allows the testing of properties expressed as Haskell functions.
Such verification is done by generating random values of the desired type,
instantiating the relevant property with them, and checking it directly by
evaluating it to a boolean. This process continues until a counterexample is
found or a specified number of cases are tested with success.
The library provides generators for several standard library
data types and combinators to build new generators for user-defined
types.

%format Int = "\D{Int}"
%format Gen = "\D{Gen}"
%format sizedRegex = "\F{sizedRegex}"
%format <*> = "\F{\,\langle\star\rangle\,}"
%format <$> = "\F{\,\langle\$\rangle\,}"
%format not = "\F{not}"
%format frequency = "\F{frequency}"
%format suchThat = "\F{suchThat}"
%format genChar = "\F{genChar}"
%format div = "\F{div}"
%format arbitrary = "\F{arbitrary}"
%format isAlphaNum = "\F{isAlphaNum}"

As an example of a custom generator, consider the task of generating a
random alpha-numeric character. To implement such generator, |genChar|, we
use QuickCheck function |suchThat| which generates a random value which satisfies
a predicate passed as argument (in example, we use |isAlphaNum|, which is true whenever
we pass an alpha-numeric character to it), using an random generator taken as input.

\begin{spec}
genChar :: Gen Char
genChar = suchThat (arbitrary :: Gen Char) isAlphaNum
\end{spec}

%format reverseInv = "\F{reverseInv}"
%format reverse = "\F{reverse}"
%format quickCheck = "\F{quickCheck}"
%format wrong = "\F{wrong}"

In its simplest form, a property is a boolean function. As an example, the following function
states that reversing a list twice produces the same input list.
\begin{spec}
reverseInv : [Int] -> Bool
reverseInv xs = reverse (reverse xs) == xs
\end{spec}

We can understand this property as been implicitly quantified universally over the argument |xs|.
Using the function |quickCheck| we can test this property over randomly generated lists:
\begin{verbatim}
  quickCheck reverseInv
  +++ OK, passed 100 tests.
\end{verbatim}
Test execution is aborted whenever a counter example is found for the tested property. For example,
consider the following wrong property about the list reverse algorithm:
\begin{spec}
wrong :: [Int] -> Bool
wrong xs = reverse (reverse xs) == reverse xs
\end{spec}
When we execute such property, a counter-example is found and printed as a result of the test.
\begin{verbatim}
  quickCheck wrong
  *** Failed! Falsifiable (6 tests and 4 shrinks).
  [0,1]
\end{verbatim}

\paragraph{Test case generators.} In order to test the correctness of our semantics, we needed to
build generators for REs and for strings. We develop functions to randomly generate strings accepted
and rejected for a RE, using the QuickCheck library.

Generation of random RE is done by function |sizedRegex|, which takes a depth limit to restrict
the size of the generated RE. Whenever the input depth limit is less or equal to 1, we can
only build a |Epsilon| or a single character RE. The definition of |sizedRegex| uses
QuickCheck function |frequency|, which receives a list of pairs formed by a weight and
a random generator and produces, as result, a generator which uses such frequency distribution.
In |sizedRegex| implementation we give a higher weight to generate characters and equal distributions
to build concatenation, union or star.

\begin{spec}
sizedRegex :: Int -> Gen Regex
sizedRegex n
  | n <= 1 = frequency [ (10, return Epsilon), (90, Chr <$> genChar) ]
  | otherwise = frequency [ (10, return Epsilon), (30, Chr <$> genChar)
         , (20, (:*:) <$> sizedRegex n2 <*> sizedRegex n2)
         , (20, (:+:) <$> sizedRegex n2 <*> sizedRegex n2)
         , (20, Star  <$> suchThat (sizedRegex n2) (not . nullable)) ]
         where n2 = div n 2
\end{spec}

For simplicity and brevity, we only generate REs that do not contain sub-REs of the form $e^\star$,
where $e$ is nullable\footnote{A RE $e$ is \emph{nullable} if $\epsilon \in \sembrackets{e}$.}.
All results can be extended to problematic\footnote{We say that a RE $e$ is problematic if there's $e'$
such that $e = e'^\star$ and $\epsilon \in\sembrackets{e'}$.} REs in the style of Frisch et. al~\cite{Frisch2004}.

%format randomMatch = "\F{randomMatch}"
%format liftM2 = "\F{liftM2}"
%format choose = "\F{choose}"
%format randomMatches = "\F{randomMatches}"
%format oneof = "\F{oneof}"
%format randomNonMatch = "\F{randomNonMatch}"
%format replicate = "\F{replicate}"
%format concat = "\F{concat}"

Given an RE $e$, we can generate a random string $s$ such that $s \in\sembrackets{e}$
using the next definition. We generate strings by choosing randomly between branches of
a union or by repeating $n$ times a string $s$ which is accepted by $e$, whenever we
have $e^\star$ (function |randomMatches|).

\begin{spec}
randomMatch :: Regex -> Gen String
randomMatch Epsilon = return ""
randomMatch (Chr c) = return [c]
randomMatch (e :*: e') = liftM2 (++) (randomMatch e)
                                     (randomMatch e')
randomMatch (e :+: e') = oneof [ randomMatch e, randomMatch e' ]
randomMatch (Star e) = do
      n <- choose (0,3) :: Gen Int
      randomMatches n e

randomMatches :: Int -> Regex -> Gen String
randomMatches m e'
  | m <= 0 = return []
  | otherwise = liftM2 (++) (randomMatch e')
                            (randomMatches (m - 1) e')
\end{spec}
The algorithm for generating random strings that aren't accepted by a RE is similarly defined.

\paragraph{Properties considered.}

%format tc = "\F{tc}"
%format all = "\F{all}"
%format flip = "\F{flip}"
%format accept = "\F{accept}"
%format absMatcherCorrect = "\F{vmCorrect}"
%format Property = "\D{Property}"
%format === = "\F{\,\equiv\,}"
%format .&&. = "\F{\,\land\,}"
%format validCode = "\F{validCode}"
%format and = "\F{and}"

In order to verify if the defined semantics is correct, we need to check the following properties:
\begin{enumerate}
   \item Our semantics accepts only and all the strings in the language described by the input RE: we
         test this property by generating random strings that should be accepted and strings that must
         be rejected by a random RE.
   \item Our semantics generates valid parsing evidence: the bit-codes produced as result have the
         following properties: 1) the bit-codes can be parsed into a valid parse tree $t$ for the random
         produced RE $e$, i.e. $\vdash t : e$ holds ; 2) |flatten t = s| and 3) |code e t = bs|.
\end{enumerate}
Note that we need a correct implementation of RE parsing to verify the first property. We use the |accept|
function from~\cite{Fischer2010} for this and compare its result with |taccept|'s. The second property demands
that the bit-codes produced can be decoded into valid parsing evidence. The verification of produced bit-codes
is done by function |validCode| shown below.


\begin{spec}
validCode :: String -> Code -> Regex -> Bool
validCode _ [] _ = True
validCode s bs e = case decode e bs of
                     Just t -> and [tc t e, flatten t == s, code t e == bs]
                     _      -> False
\end{spec}

Finally, function |absMatcherCorrect| combines both properties mentioned above into a function that
is called to test the semantics implementation.

\begin{spec}
absMatcherCorrect :: Regex -> String -> Property
absMatcherCorrect e s
  = let (r,bs) = taccept s e
    in (accept e s === r) .&&. validCode s bs e
\end{spec}

In addition to coding / decoding of parse trees, we need a function which checks if a tree is indeed a
parsing evidence for some RE $e$. Function |tc| takes, as arguments, a parse tree $t$ and a RE $e$ and
verifies if $t$ is an evidence for $e$.

\begin{spec}
tc :: Tree -> Regex -> Bool
tc TUnit Epsilon = True
tc (TChar c) (Chr c') = c == c'
tc (t :*: t') (e :*: e') = tc t e && tc t' e'
tc (InL t) (e :+: _) = tc t e
tc (InR t') (_ :+: e') = tc t' e'
tc (TList ts) (Star e) = all (flip tc e) ts
\end{spec}

Function |tc| is a implementation of parsing tree typing relation, as specified by the following
result.
\begin{Theorem}
For all tree $t$ and RE $e$, $\vdash t : e$ if, and only if, |tc t e = True|.
\end{Theorem}

\begin{proof}
  $(\to)$: We proceed by induction on the derivation of $\vdash t : e$.
  \begin{enumerate}
     \item Case rule $T1$: Then, $e = \epsilon$ and |t = TUnit| and conclusion follows.
     \item Case rule $T2$: Then, $e = a$ and |t = TChar a| and conclusion follows.
     \item Case rule $T3$: Then, $e = e_1 + e_2$ and |t = InL tl|, where $\vdash tl : e_1$.
           By induction hypothesis, we have that |tc tl e1 = True| and conclusion follows.
     \item Case rule $T4$: Then, $e = e_1 + e_2$ and |t = InR tr|, where $\vdash tr : e_2$.
           By induction hypothesis, we have that |tc tr e2 = True| and conclusion follows.
     \item Case rule $T5$: Then, $e = e_1\:e_2$ and |t = tl :*: tr|. Conclusion is immediate
           from the induction hypothesis.
     \item Case rule $T6$: Then, $e = e_1^\star$ and |t = TList ts| and conclusion follows
           from the induction hypothesis on each element of |ts|.
  \end{enumerate}
  $(\leftarrow)$: We proceed by induction on $e$.
  \begin{enumerate}
     \item Case $e = \epsilon$: Then, |t = TUnit| and the conclusions follows by rule $T1$.
     \item Case $e = a$: Then, |t = TChar a| and the conclusions follows by rule $T2$.
     \item Case $e = e_1 + e_2$: Now, we consider the following subcases:
     \begin{enumerate}
        \item Case |t = InL tl|: By induction hypothesis, we have that |tc tl e1 = True| and conclusion follows.
        \item Case |t = InR tr|: By induction hypothesis, we have that |tc tr e2 = True| and conclusion follows.
     \end{enumerate}
     \item Case $e = e_1\,e_2$: Then, |t = tl :*: tr| and conclusion follows by the induction
      hypothesis and the rule $T5$.
     \item Case $e = e_1^\star$: Then, |t = TList ts| and conclusion follows by induction hypothesis
      on each element of |ts| and rule $T6$.
  \end{enumerate}
\end{proof}

\paragraph{Code coverage results.}

After running thousands of well-succeeded tests, we gain a high degree of confidence in the correctness
of our semantics, however, it is important to measure how much of our code is covered by the test suite.
We use the Haskell Program Coverage tool (HPC)~\cite{Gill2007} to generate statistics about the execution of our tests.
Code coverage results are presented in Figure~\ref{figure:coverage}.

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{coverage-results.png}
  \caption{Code coverage results}
  \label{figure:coverage}
\end{figure}

Our test suite give us almost 100\% of code coverage, which provides a strong evidence that our semantics
is indeed correct. All top level definitions and function alternatives are actually executed by the test cases
and just two expressions are marked as non-executed by HPC.

\subsection{Coq formalization}\label{subsection:coq-formalization}

In this section we briefly present the status of our Coq formalization. First, we give a suscint introduction
to Coq focusing on features used in our formalization and next we describe the implementation of our
verified interpreter for the proposed semantics (Figure~\ref{figure:smallstep}).

\subsubsection{A tour of Coq proof assistant}

%format Set = "\TC{Set}"
%format Prop = "\TC{Prop}"
%format Type = "\TC{Type}"
%format Type1 = "\TC{Type_1}"
%format Typen = "\TC{Type_n}"
%format Typen1 = "\TC{Type_{n + 1}}"

%format . = "."
%format forall = "\mathkw{\forall}"
%format Inductive = "\mathkw{Inductive}"
%format Fixpoint = "\mathkw{Fixpoint}"
%format match = "\mathkw{match}"
%format with = "\mathkw{with}"
%format end = "\mathkw{end}"
%format Theorem = "\mathkw{Theorem}"
%format Proof = "\mathkw{Proof}"
%format Qed = "\mathkw{Qed}"
%format nat = "\D{nat}"
%format ZO = "\C{O}"
%format SS = "\C{S}"
%format plus = "\F{plus}"
%format plus0r = "\F{plus\_0\_r}"
%format plus0r1 = "\F{plus\_0\_r1}"
%format natind = "\F{nat\_ind}"
%format eqrefl = "\C{eq\_refl}"
%format eqindr = "\C{eq\_ind\_r}"

Coq is a proof assistant based on the calculus of inductive
constructions (CIC) \cite{Bertot2010}, a higher-order typed
$\lambda$-calculus extended with inductive definitions. Theorem
proving in Coq follows the ideas of the so-called
``BHK-correspondence''\footnote{Abbreviation of Brower, Heyting,
  Kolmogorov, de Bruijn and Martin-L\"of Correspondence. This is also
  known as the Curry-Howard ``isomorphism''.}, where types represent
logical formulas, $\lambda$-terms represent proofs, and the task of
checking if a piece of text is a proof of a given formula corresponds
to type-checking (i.e.~checking if the term that represents the proof
has the type corresponding to the given formula) \cite{Sorensen2006}.

Writing a proof term whose type is that of a logical formula can be
however a hard task, even for simple propositions.  In order to make
this task easier, Coq provides \emph{tactics}, which are commands that
can be used to help the user in the construction of proof terms.

In this section we provide a brief overview of Coq. We start with the
small example, that uses basic
features of Coq --- types, functions and proof definitions.  In this
example, we use an inductive type that represents natural numbers in
Peano notation. The |nat| type definition includes an
annotation, that indicates that it belongs to the |Set|
sort\footnote{Coq's type language classifies new inductive (and
  co-inductive) definitions by using sorts. |Set| is the sort
  of computational values (programs) and |Prop| is the sort of
  logical formulas and proofs.}. Type |nat| is formed by two
data constructors: |ZO|, that represents the number $0$, and |SS|,
the successor function.
\begin{spec}
Inductive nat : Set :=
| ZO : nat
| SS : nat -> nat.

Fixpoint plus (n m : nat) : nat :=
   match n with
   | O => m
   | S n' => S (plus n' m)
   end.

Theorem plus0r : forall n, plus n 0 = n.
Proof.
   intros n. induction n as [| n'].
   reflexivity.
   simpl. rewrite -> IHn'. reflexivity.
Qed.
\end{spec}
Command |Fixpoint| allows the definition of functions by
structural recursion. The definition of |plus|, for summing two
values of type |nat|, is straightforward. It should be noted
that all functions defined in Coq must be total.

Besides declaring inductive types and functions, Coq allows us to
define and prove theorems. In our exemple, we show a simple theorem
about |plus|, that states that
|plus n 0 = n|, for an arbitrary value |n| of type
|nat|. Command |Theorem| allows the statement of a
formula that we want to prove and starts the \emph{interactive proof
  mode}, in which tactics can be used to produce the proof term that
is the proof of such formula. In an interactive section of Coq, after
enunciation of theorem |plusOr|, we must prove the
following goal:
\begin{spec}
 =====================
   forall n : nat, plus n 0 = n
\end{spec}
After command |Proof|, we can use tactics to build, step by
step, a term of the given type. The first tactic, |intros|, is
used to move premisses and universally quantified variables from the
goal to the hypothesis. As a result of using |intros|, the
quantified variable |n| is moved from the goal to the
hypothesis, resulting in the following:
\begin{spec}
 n : nat
=====================
 plus n 0 = n
\end{spec}
In order to prove goal |plus n 0 = n| we can proceed by induction
over the structure of |n|, by using tactic |induction|.
This generates one goal for each constructor of type |nat|
(|ZO| and |SS|), leaving us with two goals to be proved:
\begin{spec}
2 subgoals

=====================
   plus 0 0 = 0

subgoal 2 is:
 plus (SS n') 0 = SS n'
\end{spec}
Goal |plus 0 0 = 0| can be shown to hold directly by the
definition of |plus|. Tactic |reflexivity| proves such
equalities, after reducing both sides of the equality to their normal
forms. The next goal to be proved is:
\begin{spec}
 n' : nat
 IHn' : plus n' 0 = n'
=====================
   plus (SS n') 0 = SS n'
\end{spec}
Tactic |induction| automatically generates the induction
hypothesis |IHn'| for this theorem. In order to finish the proof,
we need to transform the goal to use the inductive hypothesis. In this
case we can simply use tactic |simpl|, for performing reductions
based on the definition of |plus|. This changes the goal to:
\begin{spec}
 n' : nat
 IHn' : plus n' 0 = n'
=====================
   SS (plus n' 0) = SS n'
\end{spec}
Since the goal now has exactly the left hand side of the hypothesis
|IHn'| as a sub-term , we can use tactic |rewrite|.  The use
of |rewrite -> IHn'| replaces |plus n' 0| by |n'|
(|rewrite <- IHn'| does the reverse, i.e.~replaces |n'| by
|plus n' 0|). We obtain now the following:
\begin{spec}
 n' : nat
 IHn' : plus n' 0 = n'
=====================
   SS n' = SS n'
\end{spec}
Goal |SS n' = SS n'| can be proven by using |reflexivity|.

This tactic script creates the term presented in Figure
\ref{term-builded-by-tactics}. For each inductively defined data type,
Coq generates automatically an induction principle \cite[Chapter
  14]{Bertot2010}. For natural numbers, the following Coq term, called
|natind|, is created:
\begin{spec}
natind
     : forall P : nat -> Prop,
       P ZO -> (forall n : nat, P n -> P (SS n)) ->
       forall n : nat, P n
\end{spec}
It expects a property (|P|) over natural numbers (a value of type
|nat -> Prop|), a proof that |P| holds for zero (a value of
type |P 0|) and a proof that if |P| holds for an arbitrary
natural |n|, then it holds for |SS n| (a value of type
|forall n:nat, P n -> P (SS n)|). Besides |natind|, generated by
the use of tactic |induction|, the term in Figure
\ref{term-builded-by-tactics} uses the constructor of the equality
type |eqrefl|, created by tactic |reflexivity|, and term
|eqindr|, inserted by the use of tactic |rewrite|. Term
|eqindr| allows concluding |P y| based on the assumptions
that |P x| and |x = y| are provable.
%format fun = "\lambda"
\begin{figure}
	\begin{spec}
	fun n : nat =>
		natind
		(fun n0 : nat => plus n0 ZO = n0) (eqrefl ZO)
		  (fun (n' : nat) (IHn' : plus n' ZO = n') =>
		   eqindr (fun n0 : nat => SS n0 = SS n')
		     (eqrefl (SS n')) IHn') n
		     : forall n : nat, plus n ZO = n
	\end{spec}
	\caption{Term that represents the proof of theorem |plus0r|.}
	\label{term-builded-by-tactics}
\end{figure}
Instead of using tactics, one could instead write CIC terms directly
to prove theorems.  This can be however a complex task, even for
simple theorems like |plus0r|, because it generally requires
detailed knowledge of the CIC type system.

%format sig = "\D{sig}"
%format exist = "\C{exist}"
%format /== = "\F{\neq}"

An interesting feature of Coq is the possibility of defining inductive
types that mix computational and logical parts. Such types are usually
called \emph{strong specifications}, since they allow the definition
of functions that compute values together with a proof that this value
has some desired property. As an example, consider type |sig|
below, also called ``subset type'', that is defined in Coq's standard
library as:
\begin{spec}
Inductive sig (A : Set)	(P : A -> Prop) : Set :=
 | exist : forall x : A, P x -> sig A P.
\end{spec}
Type |sig| is usually expressed in Coq by using the following
syntax: $\{x : A \,\vert\,P\:x\}$
Constructor |exist| has two
parameters. Parameter |x| of type |A| represents the
computational part. The other parameter, of type |P x|, denotes
the ``certificate'' that |x| has the property specified by
predicate |P|. As an example, consider:
\begin{spec}
forall n : nat, n /== ZO -> {p | n = SS p}.
\end{spec}
This type can be used to specify a function that returns the
predecessor of a natural number |n|, together with a proof that
the returned value really is the predecessor of |n|. The
definition of a function of type |sig| requires the specification
of a logical certificate. As occurs in the case of theorems, tactics
can be used in the definition of such functions. For example, a
definition of a function that returns the predecessor of a given
natural number, if it is different from zero, can be given as follows:
%format predcert = "\F{pred\_cert}"
%format Definition = "\mathkw{Definition}"
%format Defined = "\mathkw{Defined}"
%format sumor = "\D{sumor}"
%format inleft = "\C{inleft}"
%format inright = "\C{inright}"
\begin{spec}
Definition predcert : forall n : nat, n /== ZO -> {p | n = SS p}.
   intros n H.
   destruct n as [| n'].
   destruct H. reflexivity.
   exists n'. reflexivity.
Defined.
\end{spec}
Tactic |destruct| is used to start a proof by case analysis on
structure of a value.

Another example of a type that can be used to provide strong
specifications in Coq is |sumor|, that is defined in the
standard library as follows:
\begin{spec}
Inductive sumor(A : Set) (B : Prop) : Set :=
| inleft : A -> sumor A B
| inright : B -> sumor A B
\end{spec}
Coq standard library also provides syntactic sugar (or, in Coq's
terminology, notations) for using this type: ``|sumor A B|'' can
be written as $A + \{B\}$.
This type can be used as the type of a function that returns either a
value of type |A| or a proof that some property specified by
|B| holds.
As an example, we can specify the type of a function that returns a
predecessor of a natural number or a proof that the given number is
equal to zero as follows, using type |sumor|:
\begin{spec}
{p | n = SS p} + {n = ZO}
\end{spec}

A common problem when using rich specifications for functions is the need
of writing proof terms inside its definition body. A possible solution for
this is to use the |refine| tactic, which allows one to specify a term with
missing parts (knowns as ``holes'') to be filled latter using tactics.

The next code piece uses the |refine| tactic to build the computational part
of a certified predecessor function. We use holes to mark positions where proofs are
expected. Such proof obligations are later filled by tactic |reflexivity| which finishes
|predcert| definition.

\begin{spec}
Definition predcert : forall n : nat, {p | n = S p} + {n = 0}.
  refine (fun n =>
            match n with
            | O => inright _
            | S n' => inleft _ (exist _ n' _)
            end) ; reflexivity.
Defined.
\end{spec}

\subsubsection{A verified interpreter for the small-step semantics}

\paragraph{RE Syntax and contexts definition.} We let the following inductive type
denote the syntax of RE.
%format list = "\D{list}"
%format regex = "\D{regex}"
%format Emp   = "\C{Emp}"
%format Eps   = "\C{Eps}"
%format Cat   = "\C{Cat}"
%format Choice = "\C{Choice}"
%format Notation = "\mathkw{Notation}"
%format at = "\mathkw{at}"
%format level = "\mathkw{level}"
%format left = "\mathkw{left}"
%format ascii = "\D{ascii}"
%format associativity = "\mathkw{associativity}"
\begin{spec}
Inductive regex : Set :=
| Emp : regex
| Eps : regex
| Chr : ascii -> regex
| Cat : regex -> regex -> regex
| Choice : regex -> regex -> regex
| Star : regex -> regex.
\end{spec}
To ease the task of writing code that manipulate |regex| values, we
introduce notations to represent its constructor in a more friendly way.
For exemple, let |e,e1 : regex|. Instead of writing |Cat e1 e2|, after defining
the notations below, we can just write |e1 @ e2|.
\begin{spec}
Notation "'#0'" := Emp.
Notation "'#1'" := Eps.
Notation "'$' c" := (Chr c)(at level 40).
Notation "e '@' e1" := (Cat e e1)
                       (at level 15, left associativity).
Notation "e ':+:' e1" := (Choice e e1)
                         (at level 20, left associativity).
Notation "e '^*'" := (Star e)(at level 40).
\end{spec}
Having defined the RE syntax, our next step is to define its data-type of
one-hole contexts.
%format hole = "\D{hole}"
%format LeftChoiceHole = "\C{LeftChoiceHole}"
%format RightChoiceHole = "\C{RightChoiceHole}"
%format LeftCatHole = "\C{LeftCatHole}"
%format RightCatHole = "\C{RightCatHole}"
%format StarHole = "\C{StarHole}"
%format ctx = "\D{ctx}"
\begin{spec}
Inductive hole : Set :=
| LeftChoiceHole : regex -> hole
| RightChoiceHole : regex -> hole
| LeftCatHole : regex -> hole
| RightCatHole : regex -> hole
| StarHole : hole.

Definition ctx := list hole.
\end{spec}
The structure of |hole| and |regex| are just translations of its
Haskell's correspondent definitions to Coq.

\paragraph{Small-step semantics.} Unlike Haskell, Coq supports full dependent types which allow us 
to specify inductively defined relations as Coq types. Representing our small-step semantics in Coq 
is just a matter of defining a data type such that its constructors denote the rules of the proposed
semantics.
%format ==*> = "\D{\Rightarrow}"
%format step = "\D{step}"
%format conf = "\D{conf}"
%format '==*>' = "\D{`\Rightarrow'}"
%format dir = "\D{dir}"
%format B = "\C{B}"
%format E = "\C{F}"
%format bit = "\D{bit}"
%format string = "\D{string}"
%format MkConf = "\C{MkConf}"
%format In = "\F{In}"
Data type |dir| denotes the direction and |conf| the configuration used by the semantics.
\begin{spec}
Inductive dir : Set :=
| B : dir | E : dir. 

Inductive conf : Set :=
  MkConf : dir -> regex -> ctx -> list bit -> string -> conf.

Notation "[[ d , e , c , b , s ]]" := (MkConf d e c b s).
\end{spec}
As usual in Coq's developments, we use a notation to ease the task of writing |conf| values. 
%format SEps = "\C{Eps}"
%format SChar = "\C{Chr}"
%format SLeftB = "\C{Left_B}"
%format SLeftE = "\C{Left_E}"
\begin{spec}
Inductive step : conf -> conf -> Prop :=
| SEps : forall c b s,
    [[ B , #1 , c , b , s ]] ==*> [[ E , #1 , c , b , s ]]
| SChar : forall c b s a,
    [[ B , $ a, c , b, (String a s) ]] ==*> [[ E, $ a, c, b, s ]]
| SLeftB : forall c b s e e' c' b',
    c' = LeftChoiceHole e' :: c ->
    b' = Zero :: b -> 
    [[ B , e :+: e', c, b, s ]] ==*> [[ B , e , c' , b', s ]]
| SLeftE : forall e e' c s b c' b',
    c' = LeftChoiceHole e' :: c ->
    b' = Zero :: b ->
    [[ E , e , c' , b , s ]] ==*> [[ E , e :+: e', c, b', s ]]
(* some constructors omitted *)
where  c ==*> c1 := (step c c1).
\end{spec}

\paragraph{Interpreter definition.} Having defined our semantics as a Coq data type,
implementing its certified interpreter is just a matter of expressing its correctness property
as the interpreter's type.

Our first step in the formalization is to define a verified version of |next| function which produces
a list of configurations reached after executing one-step from a given input |Conf|. The Coq type for
|next| is as follows:

\begin{spec}
Definition next (c : conf) : {cs : list conf | forall c', In c' cs -> c ==*> c'}.
\end{spec}

Such type specifies that from a given configuration (|c : conf|) we produce a list |cs| of configurations that
can be reached in one step from |c| (i.e. such that for any |c'|$\in$ |cs|, we have that |c ==*> c'|). Since
such function definition involve proof terms, we use the |refine| tactic to mark proof positions as holes to
be filled latter with tactics. Below, we show parts of the |next| definition.

%format ascii_dec = "\F{ascii\_dec}"
%format :: = "\C{::}"
%format nil = "\C{nil}"
\begin{spec}
  refine (match c with
          | [[ B, #1 , co , b, s ]] =>
            exist _ ([[ E, #1, co, b, s ]] :: nil) _
          | [[ B, $ a, co, b , (String a' s) ]] =>
            if ascii_dec a a' then 
              exist _ ([[ E , $ a, co, b, s ]] :: nil) _
            else exist _ (@nil conf) _
          | [[ B , e :+: e', c , b, s ]] =>
            exist _ ( [[ B, e , LeftChoiceHole e' :: c, Zero :: b, s ]] ::
                      [[ B , e' , RightChoiceHole e :: c, One :: b, s ]] :: nil) _
          (* some code omitted *)
          end) (* tactics omitted*) 
\end{spec}

The first equation of |next| definition shows the code for the semantics rule $Eps$: Given a configuration
|[[ B, #1 , co , b, s ]]|, the unique possible result is a singleton list containing |[[ E, #1, co, b, s ]]|.
Second equation deals with single character REs and it specifies that if the input string starts with the same
character as the RE's, the result should be a list containing only the configuration |[[ E , $ a, co, b, s ]]|;
otherwise an empty list is returned. Whenever the input |conf| is of the form |[[ B , e :+: e', c , b, s ]]|,
we need to output the two possible resulting configurations: one for the left-hand side of the input RE:
\begin{spec}
   [[ B, e , LeftChoiceHole e' :: c, Zero :: b, s ]]
\end{spec}
which corresponds to rule $Left_B$ and the configuration equivalent to rule $Right_B$
\begin{spec}
   [[ B , e' , RightChoiceHole e :: c, One :: b, s ]]
\end{spec}
The equations for other rules follows a similar pattern and are omited for brevity.

%format ==*>* = "\D{\Rightarrow^\star}"
%format /\ = "\D{\land}"
%format exists = "\D{\exists}"
%format nexts_type = "\F{nexts\_type}"
%format nexts = "\F{nexts}"

Having defined the function |next|, we need to compute its reflexive-transtive closure.
We let notation |==*>*| denote the reflexive-transitive closure of the relation |step|.
In order to build the closure of |next| function, we use some auxiliar definitions.

First, we define |nexts_type| as a type for a function that returns a list of all
accessible configurations, in one-step of execution, from a given input list of |conf|'s.
\begin{spec}
Definition nexts_type (cs : list conf) :=
  {cs' | forall c', In c' cs' -> exists c, In c cs /\ c ==*> c'} + {cs = []}.
\end{spec}
We name such function as |nexts| and we omit its |refine| tactic based definition.
%format steps = "\F{steps}"
Finally, our certified interpreter is built using |nexts| function and its type is presented below:
\begin{spec}
Definition steps : forall (cs : list conf)(n : nat), 
    {css | forall c', In c' css -> exists c, In c cs /\ c ==*>* c'} + {cs = []}.
\end{spec}
Again, we use |refine| tactic to construct such function. Its type specifies that it returns a list
configurations that can be reached after zero or more steps from the input configuration list. It is
worth to mention that such function is defined by recursion over a ``fuel'' parameter~\cite{McBride15}
to ensure its structurally recursive definition. Currently, we are working on defining |steps| function
using well-founded relations or even domain predicates~\cite{BoveKS16}.

\section{Related work}\label{section:related}

Ierusalimschy \cite{Ierusalimschy2009} proposed the use of Parsing Expression Grammars (PEGs) as a basis
for pattern matching. He argued that pure REs is a weak formalism for pattern-matching tasks:
many interesting patterns either are difficult to to describe or cannot be described by REs. He also said
that the inherent non-determinism of REs does not fit the need to capture specific parts of a match. Following
this proposal, he presented LPEG, a pattern-matching tool based on PEGs for the Lua language. He
argued that LPEG unifies the ease of use of pattern-matching tools with the full expressive power of PEGs.
He also presented a parsing machine (PM) that allows an implementation of PEGs for pattern matching.
In~\cite{Medeiros2008}, Medeiros et. al. presents informal correctness proofs of LPEG PM.
While such proofs represent a important step towards the correctness of LPEG, there is no guarantee that LPEG
implementation follows its specification.

In \cite{Rathnayake2011}, Rathnayake and Thielecke formalized a VM implementation for RE matching using
operational semantics. Specifically, they derived a series of abstract machines, moving from the abstract
definition of matching to realistic machines. First, a continuation is added to the operational semantics
to describe what remains to be matched after the current expression. Next, they represented the expression
as a data structure using pointers, which enables redundant searches to be eliminated via testing for pointer
equality. Although their work has some similarities with ours (a VM-based parsing of REs), they did not present
any evidence or proofs that their VM is correct.

Fischer, Huch and Wilke \cite{Fischer2010} developed a Haskell program for matching REs. The program is purely
functional and it is overloaded over arbitrary semirings, which solves the matching problem and supports other
applications like computing leftmost longest matchings or the number of matchings. Their program can also be used
for parsing every context-free language by taking advantage of laziness. Their developed program is based on an
old technique to turn REs into finite automata, which makes it efficient compared to other similar approaches.
One advantage of their implementation over our proposal is that their approach works with context-free languages,
not only with REs purely. However, they did not present any correctness proofs of their Haskell code.

Cox~\cite{Cox2009} said that viewing RE matching as executing a special machine makes it possible to add new
features just by the inclusion of new machine instructions. He presented two different ways to implement
a VM that executes a RE that has been compiled into  byte-codes: a recursive and a non-recursive
backtracking implementation, both in C programming language. Cox's work on VM-based RE parsing is poorly specified:
both the VM semantics and the RE compilation process are described only informally
and no correctness guarantees is even mentioned.

Frisch and Cardelli~\cite{Frisch2004} studied the theoretical problem of matching a flat sequence against a type (RE): the
result of the process is a structured value of a given type. Their contributions were in noticing that: (1) A disambiguated
result of parsing can be presented as a data structure that does not contain ambiguities. (2) There are problematic cases in
parsing values of star types that need to be disambiguated. (3) The disambiguation strategy used in XDuce and CDuce (two
XML-oriented functional languages) pattern matching can be characterized mathematically by what they call greedy RE matching.
(4) There is a linear time algorithm for the greedy matching. Their approach is different since they want to axiomatize abstractly
the disambiguation policy, without providing an explicit matching algorithm. They identify three notions of problematic words, REs,
and values (which represent the ways to match words), relate these three notions, and propose matching algorithms to deal with the
problematic case.

Ribeiro and Du Bois~\cite{Ribeiro2017} described the formalization of a RE parsing algorithm that produces a bit representation
of its parse tree in the dependently typed language Agda. The algorithm computes bit-codes using Brzozowski derivatives and
they proved that the produced codes are equivalent to parse trees ensuring soundness and completeness with respect to an
inductive RE semantics. They included the certified algorithm in a tool developed by themselves, named verigrep, for RE-based
search in the style of GNU grep. While the authors provide formal proofs, their tool show a bad performance when compared with
other approaches to RE parsing. Besides, they did not prove that their algorithm follows some disambiguation policy, like POSIX
or greedy.

Nielsen and Henglein~\cite{Lasse2011} showed how to generate a compact \textit{bit-coded} representation of a parse tree for a
given RE efficiently, without explicitly constructing the parse tree first, by simplifying the DFA-based parsing algorithm of
Dub\'e and Feeley~\cite{Dube2000} to emit a bit representation without explicitly materializing the parse tree itself.
They also showed that Frisch and Cardelli’s greedy RE parsing algorithm \cite{Frisch2004} can be straightforwardly modified to
produce bit codings directly. They implemented both solutions as well as a backtracking parser and performed benchmark experiments
to measure their performance. They argued that bit codings are interesting in their own right since they are typically not
only smaller than the parse tree, but also smaller than the string being parsed and can be combined with other techniques for
improved text compression. As others related works, the authors did not present a formal verification of their implementations.

An algorithm for POSIX RE parsing is described in~\cite{Sulzmann14}. The main idea of the article is to adapt
derivative parsing to construct parse trees incrementally to solve both matching and submatching for REs. In order to improve the
efficiency of the proposed algorithm, Sulzmann et al. use a bit encoded representation of RE parse trees. Textual proofs of
correctness of the proposed algorithm are presented in an appendix.

\section{Conclusion}\label{section:conclusion}

In this work, we presented a small-step operational semantics for a virtual machine for RE parsing inspired on
Thompson's NFA construction. Our semantics produces, as parsing evidence, bit-codes which can be used to characterize which
disambiguation strategy is followed by the semantics. We use data-type derivatives to represent evaluation contexts for RE.
Such contexts are used to decide how to finish the execution of the RE on focus. We have developed a prototype implementation
of our semantics in Haskell and use QuickCheck to verify its relevant properties with respect to a simple implementation
of RE parsing by Fisher et. al.~\cite{Fischer2010}.

Currently, we have a formalized interpreter of our semantics in Coq proof assistant~\cite{Bertot2010} available at project's
on-line repository~\cite{regexvm-rep}. We are working on formalizing the equivalence between the proposed semantics and
the standard RE inductive semantics.

As future work we intend to use our verified semantics to build a certified tool for RE
parsing, work on proofs that the semantics follow a specific disambiguation strategy and investigate how other algorithms
(e.g. the Glushkov construction~\cite{Gluskov1961}) for converting a RE into a finite state machine could be expressed in
terms of an operational semantics.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\appendix

\section{Correctness of the |accept| function}\label{appendix:accept}

%format parts = "\F{parts}"
%format split = "\F{split}"

Fisher et. al.~\cite{Fischer2010} presents a simple and elegant function for parsing a string using a RE. It relies on two
auxiliar functions that break an input string into its parts. The first is function |split| which decompose the input string in
a prefix and a suffix.
\begin{spec}
split :: [a] -> [([a],[a])]
split [] = [([],[])]
split (c:cs) = ([], c : cs) : [(c : s1 , s2) | (s1,s2) <- split cs] 
\end{spec}
Function |split| has the following correctness property.
\begin{Lemma}
 Let |xs| be an arbitrary list. For all |ys|, |zs| such that |(ys,zs)| $\in$ |split xs|, we have that |xs == ys ++ zs|.
\end{Lemma}
\begin{proof}
   By induction on the structure of |xs|. 
\end{proof}
Function |parts| decomposes a string into a list of its parts. Such property is expressed by the following lemma.
\begin{Lemma}
Let |xs| be an arbitrary list. For all |yss| such that |yss| $\in$ |parts xs|, we have that |concat yss == xs|.
\end{Lemma}
\begin{proof}
   By induction on the structure of |xs|.
\end{proof}
Finally, function |accept| is defined by recursion on the input RE using functions |parts| and |split| in the
Kleene star and concatenation cases. The correctness of |accept| states that it returns true only when the
input string is in input RE's language, as stated in the next theorem.
\begin{Theorem}
   For all |s| and |e|, |accept e s == True| if, and only if, $s\in\sembrackets{e}$.
\end{Theorem}
\begin{proof}
   $\,$\\
   \begin{itemize}
      \item[$(\to)$]: By induction on the structure of |e| using lemmas about |parts| and |split|.
      \item[$(\leftarrow)$]: By induction on the derivation of $s \in\sembrackets{e}$.
   \end{itemize}
\end{proof}
\end{document}
